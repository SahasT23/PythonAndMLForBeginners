{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4332e88",
   "metadata": {},
   "source": [
    "# Need to use the last bit of code and work back from there\n",
    "\n",
    "1. Need to explain log reg\n",
    "2. how it differs from linear\n",
    "3. build model from ground up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f4b12a",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "## Logistic Regression: A Complete Guide for Beginners\n",
    "\n",
    "This guide explains **logistic regression** from the ground up, covering its mechanics, equations, training process, regularization, and tuning. It includes a Python implementation from scratch with L2 regularisation.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Logistic Regression?\n",
    "\n",
    "Logistic regression is a **machine learning algorithm** for **binary classification**, predicting whether an input belongs to one of two classes (e.g., spam vs. not spam, sick vs. healthy). It outputs a **probability** (between 0 and 1) that an input belongs to the positive class. For example, it might predict a 75% chance that a patient has a disease, and we classify based on a threshold (typically 0.5).\n",
    "\n",
    "Unlike **linear regression**, which predicts continuous values, logistic regression ensures outputs are probabilities using the **sigmoid function**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step-by-Step Explanation\n",
    "\n",
    "### 1. From Linear to Logistic Regression\n",
    "\n",
    "Logistic regression builds on linear regression, where a continuous output $ y $ is predicted as a linear combination of features $ x_1, x_2, \\dots, x_n $:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n\n",
    "$$\n",
    "\n",
    "- $ w_0 $: Intercept (bias).\n",
    "- $ w_1, w_2, \\dots, w_n $: Weights for each feature.\n",
    "\n",
    "However, for classification, we need probabilities between 0 and 1. Linear regression can produce negative or large values, unsuitable for probabilities. Logistic regression applies the **sigmoid function** to map the linear combination to [0, 1].\n",
    "\n",
    "#### Sigmoid Function\n",
    "\n",
    "The sigmoid function is:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- $ z $: Input (linear combination).\n",
    "- $ e $: Base of natural logarithm (≈ 2.718).\n",
    "- Output: Between 0 and 1 (probability-like).\n",
    "\n",
    "Properties:\n",
    "- Large positive $ z $: $ \\sigma(z) \\approx 1 $.\n",
    "- Large negative $ z $: $ \\sigma(z) \\approx 0 $.\n",
    "- $ z = 0 $: $ \\sigma(z) = 0.5 $.\n",
    "\n",
    "#### Logistic Regression Model\n",
    "\n",
    "For input features $ \\mathbf{x} = [x_1, x_2, \\dots, x_n] $, compute the linear combination:\n",
    "\n",
    "$$\n",
    "z = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\mathbf{w}^T \\mathbf{x} + w_0\n",
    "$$\n",
    "\n",
    "- $ \\mathbf{w} = [w_1, w_2, \\dots, w_n] $: Weight vector.\n",
    "- $ w_0 $: Bias.\n",
    "\n",
    "The predicted probability of the positive class (y=1) is:\n",
    "\n",
    "$$\n",
    "p(y=1|\\mathbf{x}) = \\sigma(z) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + w_0)}}\n",
    "$$\n",
    "\n",
    "The probability of the negative class (y=0) is:\n",
    "\n",
    "$$\n",
    "p(y=0|\\mathbf{x}) = 1 - p(y=1|\\mathbf{x}) = 1 - \\sigma(z)\n",
    "$$\n",
    "\n",
    "To classify:\n",
    "- If $ p(y=1|\\mathbf{x}) \\geq 0.5 $, predict class 1.\n",
    "- If $ p(y=1|\\mathbf{x}) < 0.5 $, predict class 0.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Training the Model\n",
    "\n",
    "To train logistic regression, we find the weights $ \\mathbf{w} $ and bias $ w_0 $ that best match predicted probabilities to true labels. This requires:\n",
    "1. A **loss function** to measure prediction errors.\n",
    "2. An **optimization algorithm** to minimize the loss.\n",
    "\n",
    "#### Loss Function: Log Loss (Binary Cross-Entropy)\n",
    "\n",
    "The **log loss** measures how well predicted probabilities match true labels. For one example with true label $ y $ (0 or 1) and predicted probability $ p = \\sigma(z) $, the loss is:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = - \\left[ y \\log(p) + (1 - y) \\log(1 - p) \\right]\n",
    "$$\n",
    "\n",
    "- If $ y = 1 $, we want $ p \\approx 1 $, so $ \\log(p) \\approx 0 $.\n",
    "- If $ y = 0 $, we want $ p \\approx 0 $, so $ \\log(1 - p) \\approx 0 $.\n",
    "- The negative sign ensures the loss is positive.\n",
    "\n",
    "For $ m $ examples, the **cost function** is the average log loss:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}, w_0) = -\\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(p^{(i)}) + (1 - y^{(i)}) \\log(1 - p^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "- $ y^{(i)} $: True label for the $ i $-th example.\n",
    "- $ p^{(i)} = \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + w_0) $: Predicted probability.\n",
    "\n",
    "#### Optimization: Gradient Descent\n",
    "\n",
    "We minimize $ J $ using **gradient descent**, which iteratively adjusts weights to reduce the loss. The gradient of the cost function with respect to each parameter tells us how to update it.\n",
    "\n",
    "Gradient for weight $ w_j $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( p^{(i)} - y^{(i)} \\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "Gradient for bias $ w_0 $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( p^{(i)} - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\n",
    "w_j \\leftarrow w_j - \\alpha \\frac{\\partial J}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0 \\leftarrow w_0 - \\alpha \\frac{\\partial J}{\\partial w_0}\n",
    "$$\n",
    "\n",
    "- $ \\alpha $: **Learning rate** (e.g., 0.01), controls step size.\n",
    "- Repeat until convergence (loss stabilizes).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Regularization: Preventing Overfitting\n",
    "\n",
    "**Overfitting** occurs when the model fits training data too well, including noise, and performs poorly on new data. **Regularization** penalizes large weights to keep the model simpler.\n",
    "\n",
    "#### L2 Regularization (Ridge)\n",
    "\n",
    "L2 regularization adds a penalty for the squared weights:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\mathbf{w}, w_0) = J(\\mathbf{w}, w_0) + \\frac{\\lambda}{2} \\sum_{j=1}^n w_j^2\n",
    "$$\n",
    "\n",
    "- $ \\lambda $: Regularization parameter (e.g., 0.1).\n",
    "- $ \\frac{\\lambda}{2} \\sum_{j=1}^n w_j^2 $: Penalty term (bias $ w_0 $ is not regularized).\n",
    "\n",
    "The gradient for $ w_j $ becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( p^{(i)} - y^{(i)} \\right) x_j^{(i)} + \\lambda w_j\n",
    "$$\n",
    "\n",
    "The gradient for $ w_0 $ is unchanged.\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "\n",
    "L1 regularization uses the absolute values of weights:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\mathbf{w}, w_0) = J(\\mathbf{w}, w_0) + \\lambda \\sum_{j=1}^n |w_j|\n",
    "$$\n",
    "\n",
    "L1 can set some weights to zero, useful for feature selection, but is less common due to non-differentiability. We’ll use **L2 regularization** in the implementation.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Building the Model\n",
    "\n",
    "Steps to build the model:\n",
    "1. Initialize weights $ \\mathbf{w} $ and bias $ w_0 $ (e.g., zeros or small random values).\n",
    "2. For each iteration of gradient descent:\n",
    "   - Compute predictions $ p^{(i)} = \\sigma(\\mathbf{w}^T \\mathbf{x}^{(i)} + w_0) $.\n",
    "   - Compute loss and gradients.\n",
    "   - Update weights and bias.\n",
    "3. Stop when loss converges or after a fixed number of iterations.\n",
    "4. Use the model to predict probabilities or classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Tuning the Model\n",
    "\n",
    "**Hyperparameters** to tune:\n",
    "- **Learning rate ($ \\alpha $)**: Controls step size (e.g., 0.001, 0.01, 0.1).\n",
    "- **Regularization parameter ($ \\lambda $)**: Balances fit and simplicity (e.g., 0.01, 0.1, 1.0).\n",
    "- **Number of iterations**: Affects convergence.\n",
    "- **Threshold**: For classification (default 0.5).\n",
    "\n",
    "Tuning process:\n",
    "1. Split data into **training** (80%) and **validation** (20%) sets.\n",
    "2. Train with different hyperparameters.\n",
    "3. Evaluate on validation set using metrics like accuracy.\n",
    "4. Select the best hyperparameters.\n",
    "5. Optionally, use **cross-validation** (e.g., 5-fold).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Evaluating the Model\n",
    "\n",
    "Evaluate on a **test set** (unseen data). Common metrics:\n",
    "- **Accuracy**: Fraction of correct predictions.\n",
    "- **Precision**: True positives / Predicted positives.\n",
    "- **Recall**: True positives / Actual positives.\n",
    "- **F1 Score**: Harmonic mean of precision and recall.\n",
    "- **ROC-AUC**: Area under the Receiver Operating Characteristic curve.\n",
    "\n",
    "---\n",
    "\n",
    "## Python Implementation from Scratch\n",
    "\n",
    "Below is a complete Python implementation with L2 regularization using NumPy. It includes training, prediction, and evaluation on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9595e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ddfea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression class\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, max_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg  # Regularization parameter\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = sigmoid(z)\n",
    "            \n",
    "            # Compute loss (log loss + L2 regularization)\n",
    "            loss = (-1 / n_samples) * np.sum(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
    "            reg_term = (self.lambda_reg / 2) * np.sum(self.weights ** 2)\n",
    "            total_loss = loss + reg_term\n",
    "            self.loss_history.append(total_loss)\n",
    "            \n",
    "            # Compute gradients\n",
    "            error = y_pred - y\n",
    "            dw = (1 / n_samples) * np.dot(X.T, error) + self.lambda_reg * self.weights\n",
    "            db = (1 / n_samples) * np.sum(error)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return sigmoid(z)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca2c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "n_features = 2\n",
    "\n",
    "# Two separable classes\n",
    "X = np.vstack([\n",
    "    np.random.randn(n_samples // 2, n_features) + [2, 2],\n",
    "    np.random.randn(n_samples // 2, n_features) + [-2, -2]\n",
    "])\n",
    "y = np.array([1] * (n_samples // 2) + [0] * (n_samples // 2))\n",
    "\n",
    "# Standardize features\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "\n",
    "# Split into train and test\n",
    "train_idx = np.random.choice(n_samples, int(0.8 * n_samples), replace=False)\n",
    "test_idx = np.setdiff1d(np.arange(n_samples), train_idx)\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression(learning_rate=0.1, lambda_reg=0.1, max_iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Accuracy: {accuracy(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Plot loss history\n",
    "plt.plot(model.loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6bfc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "lambda_regs = [0.01, 0.1, 1.0]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for lam in lambda_regs:\n",
    "        model = LogisticRegression(learning_rate=lr, lambda_reg=lam, max_iterations=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy(y_test, y_pred)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_params = (lr, lam)\n",
    "\n",
    "print(f\"Best Parameters: Learning Rate = {best_params[0]}, Lambda = {best_params[1]}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a046a20",
   "metadata": {},
   "source": [
    "# Need to explain for the code below: How does it work, etc. Show the mathematics etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d592f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Manual standardization\n",
    "def standardize_features(X):\n",
    "    \"\"\"\n",
    "    Standardize features to mean 0 and standard deviation 1.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Input features (n_samples, n_features)\n",
    "    \n",
    "    Returns:\n",
    "    - X_std: Standardized features\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std[std == 0] = 1  # Avoid division by zero\n",
    "    X_std = (X - mean) / std\n",
    "    return X_std\n",
    "\n",
    "# Linear regression class with L1/L2 regularization and gradient descent variants\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, regularization='l2', \n",
    "                 gd_type='batch', batch_size=32, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initialize linear regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        - learning_rate: Step size for gradient descent\n",
    "        - lambda_reg: Regularization strength\n",
    "        - regularization: 'l1' for Lasso, 'l2' for Ridge\n",
    "        - gd_type: 'batch', 'stochastic', or 'mini-batch'\n",
    "        - batch_size: Size of mini-batches (for mini-batch GD)\n",
    "        - max_iterations: Maximum number of iterations\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.regularization = regularization.lower()\n",
    "        self.gd_type = gd_type.lower()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iterations = max_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def compute_loss(self, X, y, y_pred):\n",
    "        \"\"\"Compute Mean Squared Error with regularization.\"\"\"\n",
    "        n_samples = len(y)\n",
    "        mse = (1 / n_samples) * np.sum((y_pred - y) ** 2)\n",
    "        if self.regularization == 'l1':\n",
    "            reg_term = self.lambda_reg * np.sum(np.abs(self.weights))\n",
    "        elif self.regularization == 'l2':\n",
    "            reg_term = (self.lambda_reg / 2) * np.sum(self.weights ** 2)\n",
    "        else:\n",
    "            raise ValueError(\"Regularization must be 'l1' or 'l2'\")\n",
    "        return mse + reg_term\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using specified gradient descent variant.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Training features (n_samples, n_features)\n",
    "        - y: Training targets (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.loss_history = []\n",
    "\n",
    "        for _ in range(self.max_iterations):\n",
    "            if self.gd_type == 'batch':\n",
    "                # Batch gradient descent: use all data\n",
    "                y_pred = np.dot(X, self.weights) + self.bias\n",
    "                loss = self.compute_loss(X, y, y_pred)\n",
    "                self.loss_history.append(loss)\n",
    "                error = y_pred - y\n",
    "                dw = (2 / n_samples) * np.dot(X.T, error)\n",
    "                if self.regularization == 'l1':\n",
    "                    dw += self.lambda_reg * np.sign(self.weights)\n",
    "                elif self.regularization == 'l2':\n",
    "                    dw += self.lambda_reg * self.weights\n",
    "                db = (2 / n_samples) * np.sum(error)\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "\n",
    "            elif self.gd_type == 'stochastic':\n",
    "                # Stochastic gradient descent: one random example\n",
    "                idx = np.random.randint(0, n_samples)\n",
    "                x_i = X[idx:idx+1]\n",
    "                y_i = y[idx:idx+1]\n",
    "                y_pred = np.dot(x_i, self.weights) + self.bias\n",
    "                loss = self.compute_loss(X, y, np.dot(X, self.weights) + self.bias)\n",
    "                self.loss_history.append(loss)\n",
    "                error = y_pred - y_i\n",
    "                dw = 2 * np.dot(x_i.T, error)\n",
    "                if self.regularization == 'l1':\n",
    "                    dw += self.lambda_reg * np.sign(self.weights)\n",
    "                elif self.regularization == 'l2':\n",
    "                    dw += self.lambda_reg * self.weights\n",
    "                db = 2 * error\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "\n",
    "            elif self.gd_type == 'mini-batch':\n",
    "                # Mini-batch gradient descent: random batch\n",
    "                indices = np.random.choice(n_samples, self.batch_size, replace=False)\n",
    "                X_batch = X[indices]\n",
    "                y_batch = y[indices]\n",
    "                y_pred = np.dot(X_batch, self.weights) + self.bias\n",
    "                loss = self.compute_loss(X, y, np.dot(X, self.weights) + self.bias)\n",
    "                self.loss_history.append(loss)\n",
    "                error = y_pred - y_batch\n",
    "                dw = (2 / self.batch_size) * np.dot(X_batch.T, error)\n",
    "                if self.regularization == 'l1':\n",
    "                    dw += self.lambda_reg * np.sign(self.weights)\n",
    "                elif self.regularization == 'l2':\n",
    "                    dw += self.lambda_reg * self.weights\n",
    "                db = (2 / self.batch_size) * np.sum(error)\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "            else:\n",
    "                raise ValueError(\"gd_type must be 'batch', 'stochastic', or 'mini-batch'\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predictions.\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "# Compute Mean Squared Error\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Prepare data from California Housing dataset\n",
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    Fetch and preprocess California Housing dataset.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test: Standardized feature matrices\n",
    "    - y_train, y_test: Continuous target values\n",
    "    \"\"\"\n",
    "    # Fetch data\n",
    "    data = fetch_california_housing()\n",
    "    X = data.data  # Features\n",
    "    y = data.target  # House prices (in $100,000s)\n",
    "\n",
    "    # Manual train-test split (80-20)\n",
    "    n_samples = len(X)\n",
    "    train_size = int(0.8 * n_samples)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx = indices[:train_size]\n",
    "    test_idx = indices[train_size:]\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    # Standardize features manually\n",
    "    X_train = standardize_features(X_train)\n",
    "    X_test = standardize_features(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Grid search for hyperparameter tuning\n",
    "def grid_search(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Perform grid search over hyperparameters.\n",
    "    \n",
    "    Returns:\n",
    "    - best_params: Best hyperparameter combination\n",
    "    - best_mse: Best test MSE\n",
    "    - results: List of all results\n",
    "    \"\"\"\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    lambda_regs = [0.01, 0.1, 1.0]\n",
    "    regularizations = ['l1', 'l2']\n",
    "    gd_types = ['batch', 'stochastic', 'mini-batch']\n",
    "    results = []\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for lam in lambda_regs:\n",
    "            for reg in regularizations:\n",
    "                for gd in gd_types:\n",
    "                    model = LinearRegression(\n",
    "                        learning_rate=lr,\n",
    "                        lambda_reg=lam,\n",
    "                        regularization=reg,\n",
    "                        gd_type=gd,\n",
    "                        batch_size=32,\n",
    "                        max_iterations=500\n",
    "                    )\n",
    "                    model.fit(X_train, y_train)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    mse_val = mse(y_test, y_pred)\n",
    "                    results.append({\n",
    "                        'learning_rate': lr,\n",
    "                        'lambda_reg': lam,\n",
    "                        'regularization': reg,\n",
    "                        'gd_type': gd,\n",
    "                        'mse': mse_val,\n",
    "                        'loss_history': model.loss_history\n",
    "                    })\n",
    "                    if mse_val < best_mse:\n",
    "                        best_mse = mse_val\n",
    "                        best_params = {'lr': lr, 'lambda': lam, 'reg': reg, 'gd': gd}\n",
    "                        best_model = model\n",
    "\n",
    "    return best_params, best_mse, results, best_model\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data()\n",
    "    print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "    # Perform grid search\n",
    "    best_params, best_mse, results, best_model = grid_search(X_train, y_train, X_test, y_test)\n",
    "    print(\"\\nBest Parameters:\")\n",
    "    print(f\"Learning Rate: {best_params['lr']}\")\n",
    "    print(f\"Lambda: {best_params['lambda']}\")\n",
    "    print(f\"Regularization: {best_params['reg']}\")\n",
    "    print(f\"Gradient Descent Type: {best_params['gd']}\")\n",
    "    print(f\"Best Test MSE: {best_mse:.4f}\")\n",
    "\n",
    "    # Plot loss for the best model\n",
    "    plt.plot(best_model.loss_history, label=f\"{best_params['gd']} GD ({best_params['reg'].upper()})\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss (MSE + Reg)\")\n",
    "    plt.title(\"Loss over Iterations (Best Model)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Compare loss curves for different GD types (using best lr, lambda, reg)\n",
    "    for gd_type in ['batch', 'stochastic', 'mini-batch']:\n",
    "        model = LinearRegression(\n",
    "            learning_rate=best_params['lr'],\n",
    "            lambda_reg=best_params['lambda'],\n",
    "            regularization=best_params['reg'],\n",
    "            gd_type=gd_type,\n",
    "            batch_size=32,\n",
    "            max_iterations=500\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        plt.plot(model.loss_history, label=f\"{gd_type} GD\")\n",
    "    \n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss (MSE + Reg)\")\n",
    "    plt.title(f\"Loss Comparison (Reg: {best_params['reg'].upper()}, λ={best_params['lambda']})\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
