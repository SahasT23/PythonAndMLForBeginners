{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23f3f35",
   "metadata": {},
   "source": [
    "# Linear Regression in Machine Learning: A Beginner's Guide\n",
    "\n",
    "This is a simple way to teach a computer to predict numbers, like guessing someone's house price based on its size. We'll explain everything step-by-step, assuming you know nothing about machine learning. We'll use Python code with only three libraries: `os`, `numpy`, and `matplotlib`. Each code snippet includes comments explaining what these libraries do.\n",
    "\n",
    "## 1. What is Linear Regression?\n",
    "\n",
    "Imagine you notice that bigger houses tend to cost more. Linear Regression is like drawing a straight line through a graph of house sizes and prices to predict the price of any house based on its size. The \"line\" is the key: it assumes the relationship between size (input) and price (output) is straight, not curvy or random.\n",
    "\n",
    "### 1.1 The Equation\n",
    "The line is described by this equation:\n",
    "\n",
    "$$ y = w_0 + w_1 \\cdot x $$ \n",
    "\n",
    "- **$y$**: The number we want to predict (e.g., house price).\n",
    "- **$x$**: The input we know (e.g., house size).\n",
    "- **$w_0$**: The \"starting point\" of the line on the graph (where it crosses the y-axis when $x = 0$).\n",
    "- **$w_1$**: The \"slope\" of the line, showing how much $y$ changes when $x$ increases by 1.\n",
    "- **Example**: If $w_0 = 50,000$ and $w_1 = 100$, then for a house of size $x = 1000$ square feet, the predicted price is $y = 50,000 + 100 \\cdot 1000 = 150,000$.\n",
    "\n",
    "### 1.2 Why a Line?\n",
    "A straight line is the simplest way to connect inputs to outputs. It’s easy to understand and works well for patterns that are roughly linear (like house size vs. price). However, real data has noise (random variations), so our line won’t be perfect—it’s a \"best guess.\"\n",
    "\n",
    "### 1.3 Goal\n",
    "We need to find the best $ w_0 $ and $ w_1 $ so our line fits the data as closely as possible. \"Fitting\" means the predicted $ y $ values are close to the actual prices.\n",
    "\n",
    "## 2. Building the Model (Theory)\n",
    "\n",
    "To find the best line, we measure how \"wrong\" our predictions are using a **loss function** called **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "- **$ n $**: Number of data points (e.g., number of houses).\n",
    "- **$ y_i $**: Actual value (e.g., real house price for the $ i $-th house).\n",
    "- **$ \\hat{y}_i $**: Predicted value (e.g., price from our line: $ \\hat{y}_i = w_0 + w_1 \\cdot x_i $).\n",
    "- **$ (y_i - \\hat{y}_i)^2 $**: Squared difference between actual and predicted values, making errors positive and emphasizing big mistakes.\n",
    "- **$ \\frac{1}{n} \\sum $**: Averages the squared errors over all data points.\n",
    "\n",
    "### Why MSE?\n",
    "- **Squares Errors**: Ensures all errors are positive and big errors (e.g., predicting $100,000 when the price is $200,000) are penalized more.\n",
    "- **Average**: Gives a single number to measure overall error, so we can compare different lines.\n",
    "\n",
    "### Finding the Best Line\n",
    "We use **Gradient Descent**, a method to tweak $ w_0 $ and $ w_1 $ to make MSE as small as possible. It’s like adjusting the line’s position and slope bit by bit until it fits the data well. The update rules are:\n",
    "\n",
    "$w_1 \\leftarrow w_1 - \\alpha \\cdot \\frac{\\partial \\text{MSE}}{\\partial w_1}$\n",
    "\n",
    "$w_0 \\leftarrow w_0 - \\alpha \\cdot \\frac{\\partial \\text{MSE}}{\\partial w_0}$\n",
    "\n",
    "- **$ \\alpha $**: Learning rate, a small number (e.g., 0.01) controlling how big each tweak is.\n",
    "- **$ \\frac{\\partial \\text{MSE}}{\\partial w_1} $** and **$ \\frac{\\partial \\text{MSE}}{\\partial w_0} $**: Gradients, showing how much MSE changes if we adjust $ w_1 $ or $ w_0 $. They point to the direction that reduces error.\n",
    "\n",
    "### Why Gradient Descent?\n",
    "- **Efficient**: Works for large datasets by making small, iterative improvements.\n",
    "- **Flexible**: Can be extended to add features like regularization (explained later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc7e63",
   "metadata": {},
   "source": [
    "## Libraries and their purpose\n",
    "\n",
    "1. Numpy is a special library in Python that we will use for speeding up a lot of our machine learning algorithms.\n",
    "2. matplotlib for plotting our output nicely (there is also seaborn, which is a bit nicer in my opinion).\n",
    "3. os is useful for file handling and saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdb85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Example with Gradient Descent\n",
    "# This script demonstrates a simple linear regression model using gradient descent.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503cdf3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2688f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        # Initialize learning rate (how fast we adjust the line) and number of iterations.\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w1 = None  # Slope (weight)\n",
    "        self.w0 = None  # Intercept (bias)\n",
    "        self.loss_history = []  # Store MSE at each iteration to track progress\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # X: Input data (e.g., house sizes), shape (n_samples, n_features)\n",
    "        # y: Target data (e.g., house prices), shape (n_samples,)\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w1 = np.zeros(n_features)  # Start with w1 = 0 for each feature\n",
    "        self.w0 = 0  # Start with w0 = 0\n",
    "\n",
    "        # Gradient Descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Predict: y_pred = w0 + w1 * X\n",
    "            y_pred = np.dot(X, self.w1) + self.w0  # np.dot: Matrix multiplication\n",
    "            # Compute MSE: (1/n) * sum((y_pred - y)^2)\n",
    "            loss = np.mean((y_pred - y) ** 2)  # np.mean: Average of array\n",
    "            self.loss_history.append(loss)\n",
    "            # Gradients: How much to adjust w1 and w0\n",
    "            dw1 = (2 / n_samples) * np.dot(X.T, (y_pred - y))  # X.T: Transpose of X\n",
    "            dw0 = (2 / n_samples) * np.sum(y_pred - y)  # np.sum: Sum of array\n",
    "            # Update w1 and w0\n",
    "            self.w1 -= self.learning_rate * dw1\n",
    "            self.w0 -= self.learning_rate * dw0\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict new y values using the learned w0 and w1\n",
    "        return np.dot(X, self.w1) + self.w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake data to test the model\n",
    "np.random.seed(42)  # np.random.seed: Ensures same random numbers each run\n",
    "X = 2 * np.random.rand(100, 1)  # 100 random house sizes (0 to 2)\n",
    "y = 4 + 3 * X.flatten() + np.random.randn(100) * 0.5  # Prices: 4 + 3x + noise\n",
    "# np.random.rand: Random numbers 0-1; np.random.randn: Random normal numbers\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot data and the fitted line\n",
    "plt.scatter(X, y, color='blue', label='Data')  # plt.scatter: Plot points\n",
    "plt.plot(X, y_pred, color='red', label='Fit')  # plt.plot: Plot line\n",
    "plt.xlabel('House Size (scaled)')\n",
    "plt.ylabel('Price (thousands)')\n",
    "plt.title('Linear Regression: House Size vs. Price')\n",
    "plt.legend()  # plt.legend: Show label key\n",
    "plt.show()  # plt.show: Display plot\n",
    "\n",
    "# Plot how MSE decreases during training\n",
    "plt.plot(model.loss_history)  # Plot list of losses\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('How Error Decreases During Training')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
